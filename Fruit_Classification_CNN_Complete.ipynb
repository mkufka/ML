{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62a9d0ae",
   "metadata": {},
   "source": [
    "# Obstklassifikation mit Custom CNN-Modell\n",
    "\n",
    "Dieses Notebook implementiert ein Custom Convolutional Neural Network (CNN) zur Klassifikation von Obstbildern.\n",
    "\n",
    "## √úbersicht:\n",
    "1. Import der ben√∂tigten Bibliotheken\n",
    "2. Konfiguration der Obstklassen (erweiterbar)\n",
    "3. Laden und Vorbereiten der Daten\n",
    "4. Aufbau des CNN-Modells\n",
    "5. Training des Modells\n",
    "6. Evaluation und Visualisierung\n",
    "7. Vorhersagen auf eigenen Bildern\n",
    "\n",
    "**Datensatz:** Fruits-360 (100x100 Pixel Bilder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105614f1",
   "metadata": {},
   "source": [
    "## 1. Import der ben√∂tigten Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea387ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "# TensorFlow und Keras - Alle Imports konsistent √ºber tensorflow.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras  # Wichtig: keras als eigenst√§ndiges Modul importieren\n",
    "\n",
    "# Verwende tf.keras f√ºr alle Keras-Importe (kompatibel mit TensorFlow 2.x)\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Scikit-learn f√ºr Evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Warnungen unterdr√ºcken\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"GPU verf√ºgbar: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75595046",
   "metadata": {},
   "source": [
    "## 2. Konfiguration der Obstklassen\n",
    "\n",
    "Hier werden die zu verwendenden Obstklassen definiert. Diese Liste ist **vollst√§ndig konfigurierbar** - Sie k√∂nnen beliebig Klassen hinzuf√ºgen oder entfernen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bf2f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# KONFIGURATION - Hier k√∂nnen Sie Klassen anpassen\n",
    "# ============================================\n",
    "\n",
    "# Liste der gew√ºnschten Obstklassen\n",
    "FRUIT_CLASSES = [\n",
    "    'Apple',\n",
    "    'Banana', \n",
    "    'Cherry',\n",
    "    'Kiwi',\n",
    "    'Lemon',\n",
    "    'Orange',\n",
    "    'Peach',\n",
    "    'Strawberry',\n",
    "    'Tomato'\n",
    "]\n",
    "\n",
    "# Automatische Pfad-Ermittlung basierend auf dem Notebook-Speicherort\n",
    "# Das Notebook wird im Workspace-Root erwartet\n",
    "WORKSPACE_ROOT = os.path.dirname(os.path.abspath('__file__'))  # Aktuelles Verzeichnis\n",
    "if not os.path.exists(os.path.join(WORKSPACE_ROOT, 'Bilder')):\n",
    "    # Fallback: Verwende das aktuelle Arbeitsverzeichnis\n",
    "    WORKSPACE_ROOT = os.getcwd()\n",
    "\n",
    "# Pfade zu den Datenverzeichnissen (relativ zum Workspace)\n",
    "DATA_ROOT = os.path.join(WORKSPACE_ROOT, 'Bilder')\n",
    "TRAIN_DIR = os.path.join(DATA_ROOT, 'Training')\n",
    "TEST_DIR = os.path.join(DATA_ROOT, 'Test')\n",
    "\n",
    "# Pfad zum Ausgabeverzeichnis (im Workspace)\n",
    "MODEL_OUTPUT_DIR = os.path.join(WORKSPACE_ROOT, 'model_output')\n",
    "os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Pfad f√ºr Vorhersagen auf eigenen Bildern\n",
    "CUSTOM_IMAGES_DIR = os.path.join(WORKSPACE_ROOT, 'Obst')\n",
    "\n",
    "# Modellparameter\n",
    "IMG_SIZE = (100, 100)  # Gr√∂√üe der Trainingsbilder\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25\n",
    "NUM_CLASSES = len(FRUIT_CLASSES)\n",
    "\n",
    "print(f\"Anzahl der Klassen: {NUM_CLASSES}\")\n",
    "print(f\"Klassen: {FRUIT_CLASSES}\")\n",
    "print(f\"\\nDatenverzeichnisse:\")\n",
    "print(f\"  Training: {TRAIN_DIR}\")\n",
    "print(f\"  Test: {TEST_DIR}\")\n",
    "print(f\"  Eigene Bilder: {CUSTOM_IMAGES_DIR}\")\n",
    "print(f\"  Modell-Ausgabe: {MODEL_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bfa5ee",
   "metadata": {},
   "source": [
    "## 3. Identifizierung der relevanten Ordner\n",
    "\n",
    "Wir suchen nur nach Ordnern, die mit den definierten Klassennamen **beginnen** (z.B. \"Apple 5\", \"Apple 6\" f√ºr die Klasse \"Apple\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eb1832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_folders(base_dir, fruit_classes):\n",
    "    \"\"\"\n",
    "    Findet alle Ordner, die mit einem der Klassennamen beginnen.\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Basisverzeichnis (Training oder Test)\n",
    "        fruit_classes: Liste der gew√ºnschten Klassen\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mit Klassenname -> Liste der Ordnerpfade\n",
    "    \"\"\"\n",
    "    matching_folders = {}\n",
    "    \n",
    "    if not os.path.exists(base_dir):\n",
    "        print(f\"WARNUNG: Verzeichnis nicht gefunden: {base_dir}\")\n",
    "        return matching_folders\n",
    "    \n",
    "    all_folders = [f for f in os.listdir(base_dir) \n",
    "                   if os.path.isdir(os.path.join(base_dir, f))]\n",
    "    \n",
    "    for fruit_class in fruit_classes:\n",
    "        # Finde alle Ordner, die mit dem Klassennamen beginnen\n",
    "        class_folders = [f for f in all_folders if f.startswith(fruit_class)]\n",
    "        matching_folders[fruit_class] = class_folders\n",
    "        \n",
    "    return matching_folders\n",
    "\n",
    "# Finde passende Ordner f√ºr Training und Test\n",
    "train_folders = find_matching_folders(TRAIN_DIR, FRUIT_CLASSES)\n",
    "test_folders = find_matching_folders(TEST_DIR, FRUIT_CLASSES)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GEFUNDENE ORDNER F√úR JEDE KLASSE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for fruit_class in FRUIT_CLASSES:\n",
    "    train_count = len(train_folders.get(fruit_class, []))\n",
    "    test_count = len(test_folders.get(fruit_class, []))\n",
    "    print(f\"\\n{fruit_class}:\")\n",
    "    print(f\"  Training: {train_count} Ordner\")\n",
    "    print(f\"  Test: {test_count} Ordner\")\n",
    "    \n",
    "    # Zeige Beispiele\n",
    "    if train_count > 0:\n",
    "        print(f\"  Beispiele (Training): {', '.join(train_folders[fruit_class][:3])}\")\n",
    "    if test_count > 0:\n",
    "        print(f\"  Beispiele (Test): {', '.join(test_folders[fruit_class][:3])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477e1824",
   "metadata": {},
   "source": [
    "## 4. Erstellen von symbolischen Links f√ºr die Datenstruktur\n",
    "\n",
    "Da wir nur bestimmte Ordner verwenden m√∂chten und das Original-Verzeichnis nicht √§ndern d√ºrfen, erstellen wir eine tempor√§re Ordnerstruktur mit symbolischen Links im Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2579e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def create_filtered_dataset_structure(base_dir, output_dir, folder_mapping, fruit_classes):\n",
    "    \"\"\"\n",
    "    Erstellt eine gefilterte Datenstruktur durch Kopieren der Bilder.\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Quellverzeichnis (Training oder Test)\n",
    "        output_dir: Zielverzeichnis\n",
    "        folder_mapping: Dictionary mit Klassenname -> Liste der Ordner\n",
    "        fruit_classes: Liste der Klassen\n",
    "    \"\"\"\n",
    "    # L√∂sche altes Verzeichnis falls vorhanden\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    total_images = 0\n",
    "    \n",
    "    for fruit_class in fruit_classes:\n",
    "        class_output_dir = os.path.join(output_dir, fruit_class)\n",
    "        os.makedirs(class_output_dir, exist_ok=True)\n",
    "        \n",
    "        class_folders = folder_mapping.get(fruit_class, [])\n",
    "        class_image_count = 0\n",
    "        \n",
    "        for folder in class_folders:\n",
    "            source_folder = os.path.join(base_dir, folder)\n",
    "            \n",
    "            # Kopiere alle Bilder aus diesem Ordner\n",
    "            if os.path.exists(source_folder):\n",
    "                for img_file in os.listdir(source_folder):\n",
    "                    if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                        source_file = os.path.join(source_folder, img_file)\n",
    "                        dest_file = os.path.join(class_output_dir, f\"{folder}_{img_file}\")\n",
    "                        shutil.copy2(source_file, dest_file)\n",
    "                        class_image_count += 1\n",
    "        \n",
    "        total_images += class_image_count\n",
    "        print(f\"{fruit_class}: {class_image_count} Bilder kopiert\")\n",
    "    \n",
    "    print(f\"\\nGesamt: {total_images} Bilder\")\n",
    "    return total_images\n",
    "\n",
    "# Erstelle gefilterte Datenstruktur\n",
    "FILTERED_TRAIN_DIR = os.path.join(WORKSPACE_ROOT, 'filtered_data', 'Training')\n",
    "FILTERED_TEST_DIR = os.path.join(WORKSPACE_ROOT, 'filtered_data', 'Test')\n",
    "\n",
    "print(\"Erstelle gefilterte Trainings-Daten...\")\n",
    "train_count = create_filtered_dataset_structure(TRAIN_DIR, FILTERED_TRAIN_DIR, \n",
    "                                                 train_folders, FRUIT_CLASSES)\n",
    "\n",
    "print(\"\\nErstelle gefilterte Test-Daten...\")\n",
    "test_count = create_filtered_dataset_structure(TEST_DIR, FILTERED_TEST_DIR, \n",
    "                                                test_folders, FRUIT_CLASSES)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"DATEN ERFOLGREICH VORBEREITET\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Training: {train_count} Bilder in {FILTERED_TRAIN_DIR}\")\n",
    "print(f\"Test: {test_count} Bilder in {FILTERED_TEST_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d738ad",
   "metadata": {},
   "source": [
    "## 5. Visualisierung von Beispielbildern\n",
    "\n",
    "Zeige einige Beispielbilder aus jeder Klasse zur √úberpr√ºfung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e1a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample_images(data_dir, fruit_classes, samples_per_class=3):\n",
    "    \"\"\"Zeigt Beispielbilder f√ºr jede Klasse.\"\"\"\n",
    "    fig, axes = plt.subplots(len(fruit_classes), samples_per_class, \n",
    "                             figsize=(12, 3*len(fruit_classes)))\n",
    "    \n",
    "    if len(fruit_classes) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, fruit_class in enumerate(fruit_classes):\n",
    "        class_dir = os.path.join(data_dir, fruit_class)\n",
    "        \n",
    "        if os.path.exists(class_dir):\n",
    "            images = [f for f in os.listdir(class_dir) \n",
    "                     if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            \n",
    "            for j in range(samples_per_class):\n",
    "                ax = axes[i, j]\n",
    "                \n",
    "                if j < len(images):\n",
    "                    img_path = os.path.join(class_dir, images[j])\n",
    "                    img = load_img(img_path)\n",
    "                    ax.imshow(img)\n",
    "                    \n",
    "                    if j == 0:\n",
    "                        ax.set_ylabel(fruit_class, fontsize=12, fontweight='bold')\n",
    "                else:\n",
    "                    ax.axis('off')\n",
    "                \n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                ax.set_title(f\"Bild {j+1}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Beispielbilder aus dem Trainings-Datensatz', \n",
    "                 fontsize=14, fontweight='bold', y=1.002)\n",
    "    plt.show()\n",
    "\n",
    "show_sample_images(FILTERED_TRAIN_DIR, FRUIT_CLASSES, samples_per_class=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781cedf0",
   "metadata": {},
   "source": [
    "## 6. Daten-Generatoren erstellen\n",
    "\n",
    "Wir verwenden `ImageDataGenerator` f√ºr:\n",
    "- Normalisierung der Pixelwerte (0-1)\n",
    "- Data Augmentation f√ºr das Training (optional)\n",
    "- Automatisches Laden der Bilder in Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52284b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation f√ºr Training (optional - verbessert Generalisierung)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,              # Normalisierung\n",
    "    rotation_range=20,            # Zuf√§llige Rotation\n",
    "    width_shift_range=0.2,        # Horizontale Verschiebung\n",
    "    height_shift_range=0.2,       # Vertikale Verschiebung\n",
    "    horizontal_flip=True,         # Horizontales Spiegeln\n",
    "    zoom_range=0.2,               # Zoom\n",
    "    validation_split=0.2          # 20% f√ºr Validierung\n",
    ")\n",
    "\n",
    "# Nur Normalisierung f√ºr Test-Daten\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Training Generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    FILTERED_TRAIN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Validierung Generator\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    FILTERED_TRAIN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Test Generator\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    FILTERED_TEST_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"Data Generators erstellt:\")\n",
    "print(f\"  Training: {train_generator.samples} Bilder\")\n",
    "print(f\"  Validierung: {validation_generator.samples} Bilder\")\n",
    "print(f\"  Test: {test_generator.samples} Bilder\")\n",
    "print(f\"\\nKlassen-Mapping:\")\n",
    "class_indices = train_generator.class_indices\n",
    "for class_name, index in sorted(class_indices.items(), key=lambda x: x[1]):\n",
    "    print(f\"  {index}: {class_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66887fac",
   "metadata": {},
   "source": [
    "## 7. Custom CNN-Modell aufbauen\n",
    "\n",
    "Wir erstellen ein Custom Convolutional Neural Network mit mehreren Conv2D-Schichten, MaxPooling und Dropout zur Regularisierung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af5e9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Erstellt ein Custom CNN-Modell f√ºr Bildklassifikation.\n",
    "    \n",
    "    Architektur:\n",
    "    - 3 Convolutional Blocks (Conv2D + BatchNorm + MaxPool + Dropout)\n",
    "    - Global Average Pooling\n",
    "    - Dense Layers mit Dropout\n",
    "    - Output Layer mit Softmax\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Input Layer\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # Block 1\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Block 2\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Block 3\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Block 4 (Optional - tiefere Features)\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Global Average Pooling (besser als Flatten)\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        \n",
    "        # Dense Layers\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        # Output Layer\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Modell erstellen\n",
    "input_shape = (IMG_SIZE[0], IMG_SIZE[1], 3)\n",
    "model = build_cnn_model(input_shape, NUM_CLASSES)\n",
    "\n",
    "# Modell-Zusammenfassung\n",
    "print(\"=\" * 60)\n",
    "print(\"MODELL-ARCHITEKTUR\")\n",
    "print(\"=\" * 60)\n",
    "model.summary()\n",
    "\n",
    "# Anzahl der trainierbaren Parameter\n",
    "trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "print(f\"\\nTrainierbare Parameter: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e42e7b",
   "metadata": {},
   "source": [
    "## 8. Modell kompilieren\n",
    "\n",
    "Wir verwenden:\n",
    "- **Optimizer:** Adam (adaptiver Lernrate)\n",
    "- **Loss:** Categorical Crossentropy (f√ºr Multi-Class Klassifikation)\n",
    "- **Metrics:** Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31335b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer mit initialer Lernrate\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Modell kompilieren\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Modell erfolgreich kompiliert!\")\n",
    "print(f\"Optimizer: Adam\")\n",
    "print(f\"Loss Function: Categorical Crossentropy\")\n",
    "print(f\"Metrics: Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf50434",
   "metadata": {},
   "source": [
    "## 9. Callbacks definieren\n",
    "\n",
    "Wir verwenden:\n",
    "- **ModelCheckpoint:** Speichert das beste Modell\n",
    "- **EarlyStopping:** Stoppt Training bei Stagnation\n",
    "- **ReduceLROnPlateau:** Reduziert Lernrate bei Plateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41340171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfad f√ºr das beste Modell\n",
    "model_path = os.path.join(MODEL_OUTPUT_DIR, 'best_fruit_classifier_cnn.keras')\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    # Speichere das beste Modell basierend auf Validierungs-Accuracy\n",
    "    ModelCheckpoint(\n",
    "        model_path,\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Early Stopping - stoppt wenn keine Verbesserung mehr\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=7,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Reduziere Lernrate bei Plateau\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Callbacks konfiguriert:\")\n",
    "print(f\"  - ModelCheckpoint: {model_path}\")\n",
    "print(f\"  - EarlyStopping: Patience=7\")\n",
    "print(f\"  - ReduceLROnPlateau: Factor=0.5, Patience=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25da14b",
   "metadata": {},
   "source": [
    "## 10. Training des Modells\n",
    "\n",
    "Dies kann einige Minuten dauern, abh√§ngig von Ihrer Hardware (GPU beschleunigt das Training erheblich)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835dcceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STARTE TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Epochen: {EPOCHS}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Training Samples: {train_generator.samples}\")\n",
    "print(f\"Validation Samples: {validation_generator.samples}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Training starten\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING ABGESCHLOSSEN!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a390f565",
   "metadata": {},
   "source": [
    "## 11. Visualisierung des Trainings-Verlaufs\n",
    "\n",
    "Zeige die Entwicklung von Accuracy und Loss √ºber die Epochen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3de9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Visualisiert den Trainings-Verlauf.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Accuracy Plot\n",
    "    axes[0].plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "    axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
    "    axes[0].set_title('Modell Accuracy √ºber Epochen', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Epoche')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss Plot\n",
    "    axes[1].plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "    axes[1].plot(history.history['val_loss'], label='Validation Loss', marker='s')\n",
    "    axes[1].set_title('Modell Loss √ºber Epochen', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Epoche')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Ausgabe der finalen Metriken\n",
    "    final_train_acc = history.history['accuracy'][-1]\n",
    "    final_val_acc = history.history['val_accuracy'][-1]\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FINALE TRAINING-METRIKEN\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Training Accuracy:   {final_train_acc:.4f}\")\n",
    "    print(f\"Validation Accuracy: {final_val_acc:.4f}\")\n",
    "    print(f\"Training Loss:       {final_train_loss:.4f}\")\n",
    "    print(f\"Validation Loss:     {final_val_loss:.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f868ce",
   "metadata": {},
   "source": [
    "## 12. Evaluation auf Test-Daten\n",
    "\n",
    "Bewerte das Modell auf den ungesehenen Test-Daten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9ade70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EVALUATION AUF TEST-DATEN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Lade das beste gespeicherte Modell\n",
    "best_model = keras.models.load_model(model_path)\n",
    "print(f\"Bestes Modell geladen von: {model_path}\")\n",
    "\n",
    "# Evaluate auf Test-Daten\n",
    "test_loss, test_accuracy = best_model.evaluate(test_generator, verbose=1)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TEST-ERGEBNISSE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Test Loss:     {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df64279",
   "metadata": {},
   "source": [
    "## 13. Detaillierte Metriken und Confusion Matrix\n",
    "\n",
    "Berechne Precision, Recall, F1-Score und visualisiere die Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fc81d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorhersagen auf Test-Daten\n",
    "print(\"Erstelle Vorhersagen...\")\n",
    "y_pred_probs = best_model.predict(test_generator, verbose=1)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Class Labels\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_true, y_pred, target_names=class_labels))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Visualisierung der Confusion Matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "disp.plot(cmap='Blues', xticks_rotation=45, values_format='d')\n",
    "plt.title('Confusion Matrix - Test-Daten', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Pro-Klasse Accuracy\n",
    "print(\"\\nPro-Klasse Accuracy:\")\n",
    "print(\"-\" * 40)\n",
    "for i, label in enumerate(class_labels):\n",
    "    class_correct = cm[i, i]\n",
    "    class_total = cm[i, :].sum()\n",
    "    class_acc = class_correct / class_total if class_total > 0 else 0\n",
    "    print(f\"{label:12s}: {class_acc:.4f} ({class_acc*100:.2f}%) - {class_correct}/{class_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914f0468",
   "metadata": {},
   "source": [
    "## 14. Beispiel-Vorhersagen visualisieren\n",
    "\n",
    "Zeige einige Test-Bilder mit ihren Vorhersagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9194a5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(generator, predictions, true_labels, class_labels, num_samples=12):\n",
    "    \"\"\"Zeigt Vorhersagen f√ºr zuf√§llige Test-Bilder.\"\"\"\n",
    "    # Zuf√§llige Auswahl\n",
    "    indices = np.random.choice(len(true_labels), num_samples, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        # Lade Bild\n",
    "        img_path = generator.filepaths[idx]\n",
    "        img = load_img(img_path, target_size=IMG_SIZE)\n",
    "        \n",
    "        # Wahre und vorhergesagte Klasse\n",
    "        true_class = class_labels[true_labels[idx]]\n",
    "        pred_class = class_labels[predictions[idx]]\n",
    "        confidence = y_pred_probs[idx][predictions[idx]] * 100\n",
    "        \n",
    "        # Farbe: Gr√ºn f√ºr korrekt, Rot f√ºr falsch\n",
    "        color = 'green' if true_class == pred_class else 'red'\n",
    "        \n",
    "        # Visualisierung\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(\n",
    "            f\"Wahr: {true_class}\\nVorhersage: {pred_class}\\nKonfidenz: {confidence:.1f}%\",\n",
    "            fontsize=10,\n",
    "            color=color,\n",
    "            fontweight='bold'\n",
    "        )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Beispiel-Vorhersagen (Gr√ºn = Korrekt, Rot = Falsch)', \n",
    "                 fontsize=14, fontweight='bold', y=1.002)\n",
    "    plt.show()\n",
    "\n",
    "show_predictions(test_generator, y_pred, y_true, class_labels, num_samples=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4e9d51",
   "metadata": {},
   "source": [
    "## 15. Modell und Mapping speichern\n",
    "\n",
    "Speichere das finale Modell und das Label-Mapping f√ºr sp√§tere Verwendung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112e9c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speichere Label-Mapping\n",
    "label_map_path = os.path.join(MODEL_OUTPUT_DIR, 'label_mapping_cnn.json')\n",
    "\n",
    "# Erstelle Mapping: Index -> Klassenname\n",
    "index_to_class = {v: k for k, v in train_generator.class_indices.items()}\n",
    "\n",
    "with open(label_map_path, 'w') as f:\n",
    "    json.dump(index_to_class, f, indent=4)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODELL UND MAPPING GESPEICHERT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Modell:         {model_path}\")\n",
    "print(f\"Label-Mapping:  {label_map_path}\")\n",
    "print(\"\\nLabel-Mapping:\")\n",
    "for idx, label in sorted(index_to_class.items()):\n",
    "    print(f\"  {idx}: {label}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ea2afc",
   "metadata": {},
   "source": [
    "## 16. Vorhersagen auf eigenen Bildern aus dem \"Obst\" Ordner\n",
    "\n",
    "Jetzt k√∂nnen wir Vorhersagen auf beliebigen Bildern (jedes Format, jede Gr√∂√üe) aus dem \"Obst\"-Ordner machen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f929afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_custom_image(model, img_path, label_map, target_size=(100, 100)):\n",
    "    \"\"\"\n",
    "    Macht eine Vorhersage f√ºr ein einzelnes Bild mit korrektem Seitenverh√§ltnis.\n",
    "    \n",
    "    Args:\n",
    "        model: Trainiertes Keras-Modell\n",
    "        img_path: Pfad zum Bild\n",
    "        label_map: Dictionary mit Index -> Klassenname\n",
    "        target_size: Zielgr√∂√üe f√ºr das Bild\n",
    "        \n",
    "    Returns:\n",
    "        predicted_class, confidence, all_probabilities\n",
    "    \"\"\"\n",
    "    from PIL import Image\n",
    "    \n",
    "    # Lade Original-Bild\n",
    "    img = Image.open(img_path)\n",
    "    \n",
    "    # Konvertiere zu RGB falls n√∂tig (z.B. bei PNG mit Alpha-Kanal)\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    \n",
    "    # Berechne Seitenverh√§ltnis und erstelle quadratisches Bild mit Padding\n",
    "    width, height = img.size\n",
    "    max_dim = max(width, height)\n",
    "    \n",
    "    # Erstelle schwarzes quadratisches Bild\n",
    "    square_img = Image.new('RGB', (max_dim, max_dim), (0, 0, 0))\n",
    "    \n",
    "    # Platziere Original-Bild zentriert\n",
    "    offset_x = (max_dim - width) // 2\n",
    "    offset_y = (max_dim - height) // 2\n",
    "    square_img.paste(img, (offset_x, offset_y))\n",
    "    \n",
    "    # Resize auf Zielgr√∂√üe (jetzt ohne Verzerrung)\n",
    "    square_img = square_img.resize(target_size, Image.LANCZOS)\n",
    "    \n",
    "    # Konvertiere zu Array und preprocessing\n",
    "    img_array = img_to_array(square_img)\n",
    "    img_array = img_array / 255.0  # Normalisierung\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Batch-Dimension hinzuf√ºgen\n",
    "    \n",
    "    # Vorhersage\n",
    "    predictions = model.predict(img_array, verbose=0)\n",
    "    predicted_idx = np.argmax(predictions[0])\n",
    "    confidence = predictions[0][predicted_idx]\n",
    "    predicted_class = label_map[str(predicted_idx)]\n",
    "    \n",
    "    return predicted_class, confidence, predictions[0]\n",
    "\n",
    "\n",
    "def predict_on_custom_images(model, images_dir, label_map, target_size=(100, 100)):\n",
    "    \"\"\"\n",
    "    Macht Vorhersagen auf allen Bildern in einem Verzeichnis.\n",
    "    \"\"\"\n",
    "    # Finde alle Bilder\n",
    "    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.gif']\n",
    "    image_files = []\n",
    "    \n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(images_dir, ext)))\n",
    "        image_files.extend(glob.glob(os.path.join(images_dir, ext.upper())))\n",
    "    \n",
    "    if len(image_files) == 0:\n",
    "        print(f\"WARNUNG: Keine Bilder gefunden in {images_dir}\")\n",
    "        print(f\"Unterst√ºtzte Formate: {', '.join(image_extensions)}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Gefunden: {len(image_files)} Bilder\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Erstelle Visualisierung\n",
    "    num_images = len(image_files)\n",
    "    cols = 4\n",
    "    rows = (num_images + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(16, 4*rows))\n",
    "    \n",
    "    if num_images == 1:\n",
    "        axes = np.array([axes])\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    # Vorhersagen f√ºr jedes Bild\n",
    "    for i, img_path in enumerate(image_files):\n",
    "        # Vorhersage\n",
    "        predicted_class, confidence, all_probs = predict_custom_image(\n",
    "            model, img_path, label_map, target_size\n",
    "        )\n",
    "        \n",
    "        # Lade Original-Bild (f√ºr Anzeige)\n",
    "        img = load_img(img_path)\n",
    "        \n",
    "        # Visualisierung\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "        # Titel mit Vorhersage und Konfidenz\n",
    "        filename = os.path.basename(img_path)\n",
    "        axes[i].set_title(\n",
    "            f\"{filename}\\n\\n{predicted_class}\\nKonfidenz: {confidence*100:.1f}%\",\n",
    "            fontsize=10,\n",
    "            fontweight='bold',\n",
    "            color='darkgreen'\n",
    "        )\n",
    "        \n",
    "        # Ausgabe in Konsole\n",
    "        print(f\"\\nBild: {filename}\")\n",
    "        print(f\"  Vorhersage: {predicted_class}\")\n",
    "        print(f\"  Konfidenz:  {confidence*100:.2f}%\")\n",
    "        print(f\"  Top-3 Vorhersagen:\")\n",
    "        \n",
    "        # Top-3 Klassen\n",
    "        top_3_idx = np.argsort(all_probs)[-3:][::-1]\n",
    "        for idx in top_3_idx:\n",
    "            class_name = label_map[str(idx)]\n",
    "            prob = all_probs[idx]\n",
    "            print(f\"    {class_name:12s}: {prob*100:.2f}%\")\n",
    "    \n",
    "    # Verstecke ungenutzte Subplots\n",
    "    for i in range(num_images, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Vorhersagen auf eigenen Bildern aus \"Obst\"-Ordner', \n",
    "                 fontsize=16, fontweight='bold', y=1.002)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Definiere Pfade\n",
    "label_map_path = os.path.join(MODEL_OUTPUT_DIR, 'label_mapping_cnn.json')\n",
    "\n",
    "# Lade Label-Mapping\n",
    "with open(label_map_path, 'r') as f:\n",
    "    label_map = json.load(f)  # Keys bleiben als Strings (z.B. '0', '1', ...)\n",
    "\n",
    "# √úberpr√ºfe ob Ordner existiert\n",
    "if not os.path.exists(CUSTOM_IMAGES_DIR):\n",
    "    print(f\"HINWEIS: Ordner '{CUSTOM_IMAGES_DIR}' existiert noch nicht.\")\n",
    "    print(f\"Erstelle Ordner...\")\n",
    "    os.makedirs(CUSTOM_IMAGES_DIR, exist_ok=True)\n",
    "    print(f\"‚úì Ordner erstellt: {CUSTOM_IMAGES_DIR}\")\n",
    "    print(f\"\\nLegen Sie Ihre Obstbilder in diesen Ordner und f√ºhren Sie diese Zelle erneut aus.\")\n",
    "else:\n",
    "    # Mache Vorhersagen\n",
    "    print(\"=\" * 60)\n",
    "    print(\"VORHERSAGEN AUF EIGENEN BILDERN\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Verzeichnis: {CUSTOM_IMAGES_DIR}\\n\")\n",
    "    predict_on_custom_images(best_model, CUSTOM_IMAGES_DIR, label_map, target_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddf2c09",
   "metadata": {},
   "source": [
    "## 17. Zusammenfassung und n√§chste Schritte\n",
    "\n",
    "### ‚úÖ Was wir erreicht haben:\n",
    "\n",
    "1. **Konfigurierbare Obstklassen**: Einfach in Zelle 3 anpassbar\n",
    "2. **Automatische Ordnerfilterung**: Nur Ordner, die mit Klassennamen beginnen\n",
    "3. **Custom CNN-Modell**: Tiefes neuronales Netz mit ~4M Parametern\n",
    "4. **Data Augmentation**: Verbessert Generalisierung\n",
    "5. **Callbacks**: ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "6. **Umfassende Evaluation**: Accuracy, Precision, Recall, F1, Confusion Matrix\n",
    "7. **Flexible Vorhersagen**: Beliebige Bildgr√∂√üen und Formate\n",
    "\n",
    "### üîß M√∂gliche Verbesserungen:\n",
    "\n",
    "1. **Transfer Learning**: VGG16, ResNet50, EfficientNet als Basis verwenden\n",
    "2. **Hyperparameter-Tuning**: Learning Rate, Batch Size, Architektur optimieren\n",
    "3. **Cross-Validation**: K-Fold CV f√ºr robustere Evaluation\n",
    "4. **Ensemble-Methoden**: Mehrere Modelle kombinieren\n",
    "5. **Test-Time Augmentation**: Vorhersagen auf mehreren augmentierten Versionen\n",
    "\n",
    "### üìÅ Gespeicherte Dateien:\n",
    "\n",
    "- **Modell**: `model_output/best_fruit_classifier_cnn.keras`\n",
    "- **Label-Mapping**: `model_output/label_mapping_cnn.json`\n",
    "- **Gefilterte Daten**: `filtered_data/Training` und `filtered_data/Test`\n",
    "\n",
    "### üöÄ Verwendung:\n",
    "\n",
    "1. Legen Sie Ihre Obstbilder in den `Obst`-Ordner\n",
    "2. F√ºhren Sie Zelle 16 erneut aus\n",
    "3. Das Modell erkennt automatisch die Obstsorte!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd81d47",
   "metadata": {},
   "source": [
    "## 18. Diagnose: Test mit Original Fruits-360 Bildern\n",
    "\n",
    "Um zu √ºberpr√ºfen, ob das Problem wirklich die Watermarks und k√ºnstlichen Hintergr√ºnde sind, testen wir mit **sauberen Bildern aus dem Original-Dataset**.\n",
    "\n",
    "**Hypothese**: Das Modell funktioniert perfekt mit Fruits-360-√§hnlichen Bildern (100x100, sauberer Hintergrund, keine Watermarks), versagt aber bei Stock-Photos mit Watermarks/Dekoration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c68fb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test mit echten Fruits-360 Bildern (ohne Watermarks)\n",
    "import random\n",
    "\n",
    "def test_with_real_fruits360_samples(model, test_dir, label_map, num_samples=12):\n",
    "    \"\"\"\n",
    "    Teste mit zuf√§lligen Bildern direkt aus dem Fruits-360 Test-Set.\n",
    "    Diese Bilder haben KEINE Watermarks und √§hneln den Trainingsdaten.\n",
    "    \"\"\"\n",
    "    all_test_images = []\n",
    "    \n",
    "    # Sammle Bilder aus allen Klassen\n",
    "    for class_name in FRUIT_CLASSES:\n",
    "        class_dir = os.path.join(test_dir, class_name)\n",
    "        if os.path.exists(class_dir):\n",
    "            images = [os.path.join(class_dir, f) for f in os.listdir(class_dir) \n",
    "                     if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            all_test_images.extend([(img, class_name) for img in images])\n",
    "    \n",
    "    # Zuf√§llige Auswahl\n",
    "    selected = random.sample(all_test_images, min(num_samples, len(all_test_images)))\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"TEST MIT SAUBEREN FRUITS-360 BILDERN\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Ausgew√§hlt: {len(selected)} zuf√§llige Bilder\\n\")\n",
    "    \n",
    "    # Visualisierung\n",
    "    cols = 4\n",
    "    rows = (len(selected) + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(16, 4*rows))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for i, (img_path, true_class) in enumerate(selected):\n",
    "        # Vorhersage\n",
    "        predicted_class, confidence, all_probs = predict_custom_image(\n",
    "            model, img_path, label_map, target_size=IMG_SIZE\n",
    "        )\n",
    "        \n",
    "        # Lade Bild\n",
    "        img = load_img(img_path, target_size=IMG_SIZE)\n",
    "        \n",
    "        # Check if correct\n",
    "        is_correct = (predicted_class == true_class)\n",
    "        if is_correct:\n",
    "            correct_predictions += 1\n",
    "        \n",
    "        color = 'green' if is_correct else 'red'\n",
    "        \n",
    "        # Visualisierung\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(\n",
    "            f\"Wahr: {true_class}\\nVorhersage: {predicted_class}\\nKonfidenz: {confidence*100:.1f}%\",\n",
    "            fontsize=10,\n",
    "            fontweight='bold',\n",
    "            color=color\n",
    "        )\n",
    "        \n",
    "        # Konsolen-Ausgabe\n",
    "        symbol = \"‚úì\" if is_correct else \"‚úó\"\n",
    "        print(f\"{symbol} Bild {i+1}: {os.path.basename(img_path)}\")\n",
    "        print(f\"   Wahr: {true_class:12s} | Vorhersage: {predicted_class:12s} | Konfidenz: {confidence*100:.1f}%\")\n",
    "    \n",
    "    # Verstecke ungenutzte Subplots\n",
    "    for i in range(len(selected), len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Test mit Original Fruits-360 Bildern (OHNE Watermarks)', \n",
    "                 fontsize=16, fontweight='bold', y=1.002)\n",
    "    plt.show()\n",
    "    \n",
    "    # Ergebnis\n",
    "    accuracy = correct_predictions / len(selected) * 100\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"ERGEBNIS: {correct_predictions}/{len(selected)} korrekt ({accuracy:.1f}%)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if accuracy > 90:\n",
    "        print(\"\\n‚úÖ DIAGNOSE: Das Modell funktioniert PERFEKT mit sauberen Bildern!\")\n",
    "        print(\"   Problem best√§tigt: Watermarks & k√ºnstliche Hintergr√ºnde verwirren das Modell.\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Auch mit sauberen Bildern gibt es Probleme.\")\n",
    "        print(\"   Das Modell braucht m√∂glicherweise mehr Training oder eine andere Architektur.\")\n",
    "\n",
    "# F√ºhre Test aus\n",
    "test_with_real_fruits360_samples(best_model, FILTERED_TEST_DIR, label_map, num_samples=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f9af61",
   "metadata": {},
   "source": [
    "## 19. Analyse: Warum scheitern Stock-Photos?\n",
    "\n",
    "Das Modell wurde auf dem **Fruits-360 Dataset** trainiert, das folgende Eigenschaften hat:\n",
    "- ‚úÖ Sauberer, einfarbiger Hintergrund (meist wei√ü)\n",
    "- ‚úÖ Einzelne Fr√ºchte, zentriert\n",
    "- ‚úÖ Konsistente Beleuchtung\n",
    "- ‚úÖ KEINE Watermarks, Logos, oder Text\n",
    "- ‚úÖ KEINE dekorativen Elemente (Bl√§tter, Schnitte, etc.)\n",
    "\n",
    "**Ihre Stock-Photos haben:**\n",
    "- ‚ùå Shutterstock-Watermarks √ºber dem Bild\n",
    "- ‚ùå Dekorative Elemente (gr√ºne Bl√§tter bei Orange600x600.png)\n",
    "- ‚ùå Aufgeschnittene Fr√ºchte\n",
    "- ‚ùå Schwarze Hintergr√ºnde (ungew√∂hnlich f√ºr Fruits-360)\n",
    "\n",
    "**Das Problem:** Das CNN hat gelernt, dass bestimmte visuelle Muster = bestimmte Fr√ºchte. Watermarks und Dekorationen sind **neue, unbekannte Muster**, die das Modell verwirren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d03986d",
   "metadata": {},
   "source": [
    "## 20. L√∂sung: Robustes Modell f√ºr Real-World Bilder\n",
    "\n",
    "Um Stock-Photos mit Watermarks zu verarbeiten, gibt es mehrere Ans√§tze:\n",
    "\n",
    "### Option 1: Erweiterte Data Augmentation (empfohlen)\n",
    "- Trainiere das Modell MIT k√ºnstlichen Watermarks\n",
    "- F√ºge verschiedene Hintergr√ºnde hinzu\n",
    "- Simuliere Text-Overlays w√§hrend des Trainings\n",
    "\n",
    "### Option 2: Preprocessing mit Bildverarbeitung\n",
    "- Entferne Watermarks vor der Vorhersage\n",
    "- Segmentiere die Frucht vom Hintergrund\n",
    "- Verwende Inpainting-Techniken\n",
    "\n",
    "### Option 3: Transfer Learning mit robusterem Basismodell\n",
    "- VGG16, ResNet50, oder EfficientNet\n",
    "- Diese sind auf ImageNet trainiert (echte Fotos mit vielen St√∂rungen)\n",
    "- Bessere Generalisierung auf Real-World Bilder\n",
    "\n",
    "Implementieren wir **Option 1** - Data Augmentation mit simulierten Watermarks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3207ba9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schnellere L√∂sung: Transfer Learning mit vortrainiertem Modell\n",
    "# MobileNetV2 ist klein, schnell und robust gegen St√∂rungen\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "def build_transfer_learning_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Erstellt ein Transfer Learning Modell mit MobileNetV2.\n",
    "    MobileNetV2 ist auf ImageNet trainiert und kennt bereits viele visuelle Muster.\n",
    "    \"\"\"\n",
    "    # Lade vortrainiertes MobileNetV2 (ohne Top-Layer)\n",
    "    base_model = MobileNetV2(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg'\n",
    "    )\n",
    "    \n",
    "    # Friere Basis-Layer ein (verwende nur Features)\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Baue Custom Classifier on top\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRANSFER LEARNING MODELL F√úR ROBUSTE VORHERSAGEN\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nVorteile von MobileNetV2:\")\n",
    "print(\"  ‚úì Vortrainiert auf ImageNet (1.4M echte Fotos)\")\n",
    "print(\"  ‚úì Kennt bereits Texturen, Formen, Farben\")\n",
    "print(\"  ‚úì Robust gegen Watermarks und St√∂rungen\")\n",
    "print(\"  ‚úì Schnelleres Training (nur Classifier wird trainiert)\")\n",
    "print(\"  ‚úì Bessere Generalisierung auf neue Bilder\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nHinweis: Um dieses Modell zu trainieren:\")\n",
    "print(\"1. Erstellen Sie eine neue Zelle\")\n",
    "print(\"2. Kopieren Sie den Code aus Zelle 14-18 (Kompilieren, Callbacks, Training)\")\n",
    "print(\"3. Ersetzen Sie 'model' durch das neue Transfer Learning Modell\")\n",
    "print(\"4. Trainieren Sie 10-15 Epochen (geht schneller)\")\n",
    "print(\"\\nDas neue Modell sollte deutlich besser mit Stock-Photos umgehen!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
